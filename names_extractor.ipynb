{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuJEF1vIb4zul37vbN8Llu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadiwyne/names_extractor/blob/main/names_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the names of authors mentioned in a book.\n",
        "\n",
        "The inspiration for this project came to me while reading The Space of Literature by Maurice Blanchot. Maurice mentions many influential authors in his work and explores the themes these authors were struggling to express in their own works. By the time I finished reading the book, I realized that I was now interested in the authors Maurice had mentioned in his work, but I forgot to jot down their names.\n",
        "\n",
        "If only there was a way to go through the text again and just extract the names of all the authors mentioned there ... Well, why not make something that does just that myself?\n",
        "\n",
        "I also aim to calculate just how many times an author was mentioned in the text; this will allow me to see which author was most important for Maurice.\n",
        "\n",
        "The first step of any project involving a Jupyter Notebook is importing the required libraries and the data itself. For the sake of simplicity, I have renamed the book of my choice to `sample_book.epub`. However, it is Maurice Blanchot's The Space of Literature."
      ],
      "metadata": {
        "id": "nK5ORCO_snZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from tika import parser\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "QwZH6EPquBL4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load stopwords and NLP model"
      ],
      "metadata": {
        "id": "nFl5Y5qpwq3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "SMxYXj2Huc0X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and clean the text"
      ],
      "metadata": {
        "id": "cOG_u--tzsJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext in [\".pdf\", \".epub\"]:\n",
        "        return parser.from_file(file_path)['content']\n",
        "    elif ext == \".txt\":\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Only .pdf, .epub, .txt are allowed.\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "KkDdF0XbztMP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract authors"
      ],
      "metadata": {
        "id": "_9nmTMBP1cUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_authors(text):\n",
        "    doc = nlp(text)\n",
        "    authors = []\n",
        "\n",
        "    # 1. Extract PERSON entities with additional checks\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'PERSON' and ent.text.istitle():\n",
        "            # Filter out single-word common terms\n",
        "            if len(ent.text.split()) == 1:\n",
        "                if ent.text.lower() not in stop_words:\n",
        "                    authors.append(ent.text)\n",
        "            else:\n",
        "                authors.append(ent.text)\n",
        "\n",
        "    # 2. Extract names using contextual patterns\n",
        "    patterns = [\n",
        "        r'(?:according to|by|writes|stated by|argued by|noted by|in)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)',\n",
        "        r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+\\'s (?:work|book|essay|theory))'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        authors += re.findall(pattern, text)\n",
        "\n",
        "    return authors"
      ],
      "metadata": {
        "id": "EwaSzPJb1jK5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Processing"
      ],
      "metadata": {
        "id": "FMMEAbAK1mvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"sample_book_2.epub\"\n",
        "raw_text = load_text(file_path)\n",
        "cleaned_text = clean_text(raw_text)"
      ],
      "metadata": {
        "id": "GCqZyJHW1nwZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract and count authors"
      ],
      "metadata": {
        "id": "3ezhz82e1rdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "author_list = extract_authors(cleaned_text)\n",
        "author_counts = Counter(author_list)"
      ],
      "metadata": {
        "id": "AnaAsjga1toZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter and save results"
      ],
      "metadata": {
        "id": "TMaeUcyE1vuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if author_counts:\n",
        "    df = pd.DataFrame(author_counts.items(), columns=['Author', 'Mentions'])\n",
        "    df = df[df['Mentions'] > 1]\n",
        "    df.sort_values('Mentions', ascending=False, inplace=True)\n",
        "\n",
        "    # Filter out common non-author terms\n",
        "    common_non_authors = {\"The\", \"This\", \"But\", \"And\", \"What\", \"For\", \"That\", \"When\"}\n",
        "    df = df[~df['Author'].isin(common_non_authors)]\n",
        "\n",
        "    df.to_excel('authors.xlsx', index=False)\n",
        "    print(f\"Found {len(df)} authors. Results saved to authors.xlsx\")\n",
        "else:\n",
        "    print(\"No authors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJU55jG_1zvA",
        "outputId": "b41df7b9-cbe8-4aa2-e6e6-315563465904"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 129 authors. Results saved to authors.xlsx\n"
          ]
        }
      ]
    }
  ]
}